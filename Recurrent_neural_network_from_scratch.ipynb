{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Recurrent_neural_network_from_scratch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM8t45DVXBLt4VQFyohQnVb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PhatHuynhTranSon99/Neural-Network-From-Scratch/blob/main/Recurrent_neural_network_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1DTjrkHwg-d"
      },
      "source": [
        "# Recurrent Neural Network from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuI8AUkUwmBg"
      },
      "source": [
        "In this notebook, I will present to you the method in which we can implement a simple recurrent neural network from scratch (RNN). The motivating task for this example is the task of sentimental analysis, where we will try to predict the sentiment of a sentence (positive or negative)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFSAhgArxEcz"
      },
      "source": [
        "An example maybe:\n",
        "\n",
        "\n",
        "\n",
        "*   I love it -> Positive with label 1\n",
        "*   I do not like it -> Negative with label 0\n",
        "*   It is great -> Positive with label 1\n",
        "*   It is awful -> Negative with label 0\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycAtzSLqxhos"
      },
      "source": [
        "## Library import\n",
        "\n",
        "In this section, we will import only the necessary library for this notebook. Since this implement is totally from scratch, the most important library to use is just numpy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WE11eli_xjO5"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cn6NW8Ys4IfI"
      },
      "source": [
        "Another important module will be Spacy as we will use this to obtain the vector embeddings for each word in a sentence. Also we will load the en_core_web_sm\n",
        "models which has all pretrained word embedding included."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i83mhmBZ4T85"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqQ6ol9fyAc0"
      },
      "source": [
        "## Dataset collection and manipulation\n",
        "\n",
        "The most important phase for any data science task is to collect the data. For sentiment analysis, we can actually find many valuable datasets available online which has been cleaned and contains many examples. However, in this notebook, we will only use a small dataset that I found on Github ([rnn-from-scratch](https://github.com/vzhou842/rnn-from-scratch/blob/master/data.py)), as an foundation.\n",
        "\n",
        "However, you can apply what yout have learned after this notebook to however big or complex datasets you can find as the principles remaing the same"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqKWOtaD2ACC"
      },
      "source": [
        "### Collection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcuBWKzvzNTN"
      },
      "source": [
        "# Traing data used for the training phase \n",
        "# of the RNN\n",
        "train_data = {\n",
        "  'good': True,\n",
        "  'bad': False,\n",
        "  'happy': True,\n",
        "  'sad': False,\n",
        "  'not good': False,\n",
        "  'not bad': True,\n",
        "  'not happy': False,\n",
        "  'not sad': True,\n",
        "  'very good': True,\n",
        "  'very bad': False,\n",
        "  'very happy': True,\n",
        "  'very sad': False,\n",
        "  'i am happy': True,\n",
        "  'this is good': True,\n",
        "  'i am bad': False,\n",
        "  'this is bad': False,\n",
        "  'i am sad': False,\n",
        "  'this is sad': False,\n",
        "  'i am not happy': False,\n",
        "  'this is not good': False,\n",
        "  'i am not bad': True,\n",
        "  'this is not sad': True,\n",
        "  'i am very happy': True,\n",
        "  'this is very good': True,\n",
        "  'i am very bad': False,\n",
        "  'this is very sad': False,\n",
        "  'this is very happy': True,\n",
        "  'i am good not bad': True,\n",
        "  'this is good not bad': True,\n",
        "  'i am bad not good': False,\n",
        "  'i am good and happy': True,\n",
        "  'this is not good and not happy': False,\n",
        "  'i am not at all good': False,\n",
        "  'i am not at all bad': True,\n",
        "  'i am not at all happy': False,\n",
        "  'this is not at all sad': True,\n",
        "  'this is not at all happy': False,\n",
        "  'i am good right now': True,\n",
        "  'i am bad right now': False,\n",
        "  'this is bad right now': False,\n",
        "  'i am sad right now': False,\n",
        "  'i was good earlier': True,\n",
        "  'i was happy earlier': True,\n",
        "  'i was bad earlier': False,\n",
        "  'i was sad earlier': False,\n",
        "  'i am very bad right now': False,\n",
        "  'this is very good right now': True,\n",
        "  'this is very sad right now': False,\n",
        "  'this was bad earlier': False,\n",
        "  'this was very good earlier': True,\n",
        "  'this was very bad earlier': False,\n",
        "  'this was very happy earlier': True,\n",
        "  'this was very sad earlier': False,\n",
        "  'i was good and not bad earlier': True,\n",
        "  'i was not good and not happy earlier': False,\n",
        "  'i am not at all bad or sad right now': True,\n",
        "  'i am not at all good or happy right now': False,\n",
        "  'this was not happy and not good earlier': False,\n",
        "}\n",
        "\n",
        "# Test data for the validation of RNN model\n",
        "# This contains example that the RNN has not seen before\n",
        "test_data = {\n",
        "  'this is happy': True,\n",
        "  'i am good': True,\n",
        "  'this is not happy': False,\n",
        "  'i am not good': False,\n",
        "  'this is not bad': True,\n",
        "  'i am not sad': True,\n",
        "  'i am very good': True,\n",
        "  'this is very bad': False,\n",
        "  'i am very sad': False,\n",
        "  'this is bad not good': False,\n",
        "  'this is good and happy': True,\n",
        "  'i am not good and not happy': False,\n",
        "  'i am not at all sad': True,\n",
        "  'this is not at all good': False,\n",
        "  'this is not at all bad': True,\n",
        "  'this is good right now': True,\n",
        "  'this is sad right now': False,\n",
        "  'this is very bad right now': False,\n",
        "  'this was good earlier': True,\n",
        "  'i was not happy and not good earlier': False,\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8WAJKrq2Fc7"
      },
      "source": [
        "### Manipulation\n",
        "\n",
        "In this section, we will disect the training examples into training data. By representing them as matrices and vectors\n",
        "\n",
        "To elaborate, each word in the sentence will be encoded as a vector (vectorized) and their label will be encoded as a number (0 for negative emotion and 1 for positive emotion). Hence, a sentence will be a list containing many vectors which are representation of the words\n",
        "\n",
        "There are many ways to encode each word including using one-hot encoding, however, to make our model more robust despite the limited number of training examples, I will take advantage of pretrained word embeddings. \n",
        "\n",
        "This method will allow us to train the RNN better as there are more knowledge baked in the embedding themselves."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQbxm6Q-44P8"
      },
      "source": [
        "### Word embeddings\n",
        "\n",
        "This is how we can obtain word embeddings using Spacy en_core_web_sm model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J90UbjxD43LL"
      },
      "source": [
        "sentence = \"hello world this is awesome\"\n",
        "\n",
        "# First we create a document object using nlp \n",
        "document = nlp(sentence)\n",
        "\n",
        "# Each item in the document object is now an entity with a \n",
        "# vectorized representation\n",
        "word_hello = document[0]\n",
        "word_hello_embedding = word_hello.vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04jHzppH5w9f"
      },
      "source": [
        "word_hello_embedding is a vector with encoded information about the word 'hello'. We can print it and also check it dimension."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXFMSHLN6Bsn",
        "outputId": "c653e49f-d9f4-4713-8cbc-fc90a8d871b1"
      },
      "source": [
        "print(f\"\\\"Hello\\\"'s vectorized representation is {word_hello_embedding}\")\n",
        "print(f\"Its dimension is {word_hello_embedding.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Hello\"'s vectorized representation is [-1.6272194  -0.33563375  0.945545   -0.4469183   2.6902642   4.3016396\n",
            " -0.824273    3.1982849   0.5284388   3.6185837   1.3415287  -2.4555066\n",
            "  0.65796274  2.110478    2.577197    1.9148287   0.6069482   0.9331498\n",
            " -2.5915394  -3.3500705  -3.4974782   1.8654583  -2.3845963   0.9036485\n",
            "  1.4803083  -3.5128365  -2.5116596  -2.5202007   1.7190666   3.5116007\n",
            " -3.3501587   2.204627   -3.0264146   1.4101822   3.1886137   3.4279332\n",
            "  1.4341421  -1.1750133   0.50860566  0.93580085 -1.9668213   1.6744696\n",
            " -3.6765428  -1.734254    1.3900673  -3.8862624  -0.50333697 -1.6206884\n",
            " -0.03179216 -0.58700883 -0.13928567 -1.9868772  -0.15296161 -0.3285142\n",
            " -2.6088018   0.82431364  2.9109895   2.4748793  -2.1238127  -2.6898267\n",
            "  3.409523   -1.2409576  -2.057255   -0.11251724 -1.0778928   0.7698482\n",
            "  1.998522   -3.7546642  -1.5513041  -2.1098228  -0.05553401  1.3901733\n",
            "  2.742693    1.7499138  -2.35433     1.7996001   1.0548267   1.4774419\n",
            " -1.785715    2.662733    0.89857805 -0.7110866   0.43566102  0.4714018\n",
            "  0.58657885  2.924724   -1.2735698  -0.5416012   2.4809532  -1.9516025\n",
            " -0.5390321  -2.2522268  -2.928589    1.2648916   0.6865956  -0.98676515]\n",
            "Its dimension is (96,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Q6K_eos6Sg2"
      },
      "source": [
        "As you can see, the vector has length 96 which is somewhat small for modern versions of word embedding. If possible, we can use Spacy's en_core_web_md model to buff the length to 300 for more baked-in information. \n",
        "\n",
        "However, it will take more time to training due to the time taken to do backpropagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxwF9GmX6tAo"
      },
      "source": [
        "Now, let's convert all the sentences we have in the train_dataset and test_dataset into vectorized forms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uMSrWcS65B8"
      },
      "source": [
        "processed_train_data = []\n",
        "\n",
        "for example in train_data:\n",
        "  # Get the sentence and the label\n",
        "  x_as_sentence = example\n",
        "  y_as_boolean = train_data[x_as_sentence]\n",
        "\n",
        "  # Convert x into list of word embedding\n",
        "  x_as_doc = nlp(x_as_sentence)\n",
        "  x_as_embeddings = [entity.vector for entity in x_as_doc]\n",
        "\n",
        "  # Convert y to binary value\n",
        "  y_as_binary = 1 if y_as_boolean is True else 0\n",
        "\n",
        "  # Insert into processed_train_data\n",
        "  processed_train_data.append(\n",
        "      {\n",
        "          'x': x_as_embeddings,\n",
        "          'y': y_as_binary\n",
        "      }\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szR3pI7H71eD"
      },
      "source": [
        "Let's verify that it works by checking the first example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KghG05QK75UU",
        "outputId": "b6e918b3-e333-4bd7-f4be-75e811174679"
      },
      "source": [
        "first_example = processed_train_data[5]\n",
        "print(first_example['x'])\n",
        "print(first_example['y'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([-1.1862527 , -0.5168541 ,  0.55137575, -1.3239889 , -1.2699106 ,\n",
            "        4.0138464 ,  1.5577921 ,  2.3985279 , -2.5761452 ,  2.9888492 ,\n",
            "        1.6186955 ,  2.5658243 ,  3.6894712 ,  1.4838963 ,  0.6751787 ,\n",
            "        6.75658   , -1.1680346 , -1.6799572 , -2.970725  ,  1.1116401 ,\n",
            "       -2.140803  , -0.7070384 , -1.9781781 , -1.3197064 , -2.3057742 ,\n",
            "        2.8701203 ,  1.167284  ,  1.6811383 , -0.21117121, -0.7254225 ,\n",
            "       -0.9812827 , -0.5519656 , -1.1937029 , -1.8989229 ,  1.7492497 ,\n",
            "        1.4318465 , -2.4354227 , -2.029803  , -3.1974297 ,  1.2980125 ,\n",
            "       -1.034874  ,  0.44772184, -1.740391  , -1.3432336 , -2.2001183 ,\n",
            "        5.4227986 , -1.520893  , -1.8390274 , -0.068088  , -2.1983423 ,\n",
            "        2.6347175 , -1.2741227 , -0.25415123,  2.5880084 ,  1.8729777 ,\n",
            "       -1.3763707 , -0.82773346,  3.4450068 ,  2.5435286 , -2.715124  ,\n",
            "        2.4196951 , -1.1905601 ,  1.6580374 ,  0.94533134,  2.1511202 ,\n",
            "       -1.2368288 , -2.337473  , -3.74741   ,  5.251529  , -1.088653  ,\n",
            "       -0.4492546 , -2.640287  ,  0.27893382, -0.3686365 ,  0.74449915,\n",
            "        0.45823118,  1.0283935 ,  5.1416817 , -3.473363  ,  0.937027  ,\n",
            "       -1.6585296 ,  1.2439464 , -1.4591411 , -0.7859875 , -0.01393308,\n",
            "        0.8198138 ,  0.04165614, -1.2668362 ,  1.1150485 , -3.1213856 ,\n",
            "       -1.1033788 , -1.4778844 , -0.11438417,  2.8131804 , -0.19127262,\n",
            "       -0.17523393], dtype=float32), array([ 0.60580873, -2.149437  , -1.8671746 , -2.5724418 ,  5.4721317 ,\n",
            "       -2.5824118 ,  1.1461749 ,  1.6967112 , -1.4497886 , -0.7421713 ,\n",
            "       -0.9233714 , -1.5486703 ,  0.10438333, -0.31367883,  1.2480445 ,\n",
            "        3.3054175 , -1.0017562 ,  2.564612  , -1.601527  , -2.1499856 ,\n",
            "       -0.29834   , -1.2205249 , -0.308352  , -0.46632046, -0.8707211 ,\n",
            "        0.15500396, -2.154919  , -1.4697763 ,  3.438485  , -4.359006  ,\n",
            "        1.396491  , -2.0096157 , -4.2950106 ,  2.6019378 , -1.3003494 ,\n",
            "       -2.598628  ,  2.9574618 , -3.1708226 , -2.8223963 ,  2.3548465 ,\n",
            "        1.2453436 , -1.5481945 , -1.622197  , -1.6591336 ,  0.5665065 ,\n",
            "        0.45806438, -0.8079424 , -0.72054917, -0.5551598 ,  0.3195957 ,\n",
            "        2.1303203 ,  4.196139  ,  0.4241137 , -2.1178398 , -0.09418815,\n",
            "       -1.1818552 ,  1.5557368 ,  1.4013005 ,  1.5178957 , -2.4428272 ,\n",
            "        1.6172884 ,  0.1271118 , -0.23282242, -1.9122078 ,  3.5508034 ,\n",
            "       -1.842581  , -0.9963341 , -0.53607345,  0.74366677,  0.6509435 ,\n",
            "       -1.6591797 , -1.3353372 ,  0.545761  , -1.0425451 ,  1.7659372 ,\n",
            "       -1.9071245 ,  4.215519  , -1.2590067 , -1.068881  ,  5.7545395 ,\n",
            "        0.51412094, -3.0975451 ,  0.84629786, -0.4505034 ,  2.652284  ,\n",
            "        1.4426845 ,  1.5447133 , -0.08845878,  4.17144   , -0.6905844 ,\n",
            "        1.8232037 ,  2.0495443 , -1.1013138 , -0.29366046, -1.8632369 ,\n",
            "        4.060156  ], dtype=float32)]\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9x21Irxx8B0N"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}